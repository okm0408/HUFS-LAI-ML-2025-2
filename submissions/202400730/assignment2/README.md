## 기본 모델 성능
- 최종 테스트 정확도: 96.95%
- 훈련 시간: 약 1분 30초

## 실험 결과
### 실험 1: [학습률 변경]
- 변경사항: 학습률을 1e-1, 3e-4로 바꾼다.
- 결과: 
* 1e-1:최종 훈련 Loss: 1.7122
최종 훈련 정확도: 38.46%
최종 테스트 정확도: 37.54%
과적합 정도: 0.92% (훈련-테스트 정확도 차이)
* 3e-4:최종 훈련 Loss: 0.1794
최종 훈련 정확도: 94.89%
최종 테스트 정확도: 95.33%
과적합 정도: -0.44% (훈련-테스트 정확도 차이)
- 분석: 
* 1e-1로 바꾼 결과로는 학습률이 과도하게 높을 경우, 최적해를 지나쳐버리는 오버슈팅 현상으로 인해 Loss가 수렴하지 못하고 성능이 크게 저하됨을 확인했다. 이는 학습률 선택의 중요성을 보여주는 결과이다.
* 3e-4로 바꾼 결과로는 테스트 정확도가 훈련 정확도보다 오히려 0.44% 더 높게 나왔다는 의미이다. 이는 모델이 훈련 데이터에 전혀 과적합되지 않았음을 보여주는 강력한 증거이다. 오히려 아직 훈련 데이터조차 완전히 다 학습하지 못한 '과소적합(Underfitting)' 상태에 가깝다고 해석할 수 있다. 즉, "배운 것보다 실전(테스트)을 더 잘 봤다"는 것은 그만큼 모델이 아직 배울 여지가 많이 남았다는 뜻이다.

### 실험 2: [활성화 함수 변경]
- 변경사항: nn.Sigmoid(), nn.Tanh()로 변경함.
- 결과:
* nn.Sigmoid(): 최종 훈련 Loss: 0.1730
최종 훈련 정확도: 95.12%
최종 테스트 정확도: 95.26%
과적합 정도: -0.14% (훈련-테스트 정확도 차이)
* nn.Tanh(): 최종 훈련 Loss: 0.1175
최종 훈련 정확도: 96.65%
최종 테스트 정확도: 96.28%
과적합 정도: 0.37% (훈련-테스트 정확도 차이)
- 분석:
* nn.Sigmoid(): Sigmoid 모델의 성능이 ReLU보다 낮게 측정된 주된 원인은, Sigmoid 함수 고유의 특성으로 인한 기울기 소실 문제로 학습 효율이 저하되었기 때문으로 분석된다.
* nn.Tanh(): Tanh 모델의 테스트 정확도는 96.28% 로, Sigmoid(95.26%)보다는 훨씬 뛰어나지만 ReLU(96.85%)보다는 약간 낮은, 딱 중간 정도의 성능을 보여줬다. Sigmoid보다 성능이 좋은 가장 큰 이유는 Tanh 함수의 출력값이 0을 중심으로 분포하기 때문이다. (Tanh: -1~1, Sigmoid: 0~1) 출력값이 0을 중심으로 맞춰져 있으면 데이터가 한쪽으로 쏠리지 않아 다음 층으로 더 균형 잡힌 신호를 전달할 수 있고, 이는 전반적으로 더 빠르고 안정적인 학습으로 이어진다.
## 결론 및 인사이트
- 가장 효과적인 개선 방법: 이번 실험 과정에서 가장 효과적인 접근법은 새로운 파라미터를 추가하기보다 기본 설정의 중요성을 이해하는 것이었다. 특히, 현대 딥러닝에서 표준으로 여겨지는 ReLU 활성화 함수와 적절한 학습률(1e-3)을 선택하는 것이 다른 어떤 변화보다 안정적이고 높은 성능을 보장했다. 이는 무분별한 튜닝보다 검증된 baseline의 각 요소가 왜 효과적인지를 이해하는 것이 우선임을 시사한다.
- 관찰된 패턴:
*  학습률의 '골디락스 원칙': 학습률은 너무 높으면(1e-1) Loss가 발산하고, 너무 낮으면(3e-4) 주어진 시간 내에 충분히 학습하지 못하는 현상을 명확히 관찰했습니다. 최적의 성능을 내는 '딱 맞는' 학습률 구간은 매우 좁으며, 이는 훈련의 성패를 가르는 가장 민감한 요소임을 확인했습니다.

* '기울기 소실' 문제의 실체: 활성화 함수 실험에서 ReLU > Tanh > Sigmoid 순으로 나타난 성능 차이는 '기울기 소실(Vanishing Gradient)' 문제의 영향을 그대로 보여주는 결과였다. 특히 층이 깊지 않음에도 Sigmoid의 성능이 눈에 띄게 저하된 것은, 이 문제가 이론뿐만 아니라 실제 모델 학습에 미치는 영향이 크다는 것을 증명한다.

- 추가 개선 아이디어: CNN 아키텍처 도입: 현재 사용한 MLP(다층 퍼셉트론)는 이미지를 1차원으로 펼쳐 처리하기에 공간적 특성을 잃는 한계가 있다. 이미지 분류에 훨씬 효과적인 것으로 알려진 합성곱 신경망(CNN) 구조를 도입한다면 정확도를 99% 이상으로 크게 향상시킬 수 있을 것이다.

